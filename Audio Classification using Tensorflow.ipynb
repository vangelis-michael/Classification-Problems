{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGeNtb6fa-nH"
   },
   "source": [
    "Audio Classifier\n",
    "========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VGaBjdiBa-nL"
   },
   "source": [
    "## Important information\n",
    "\n",
    "* Choose a group name/number and rename the file with it.\n",
    "* Be sure to enter the names of every member in the group in the cell below.\n",
    "* Use the power of number and work together as a team.\n",
    "* You can employ the use of the HPC (high perfomance computer) provided your server account has been established.\n",
    "* The tutor is always available to give more explanation and assistance whenever/wherever required, but make sure you have made effort yourself.\n",
    "* Please remove all test codes.\n",
    "* The solutions to the tasks should be turn in on Friday 13th Dec, end of the day (12am). \n",
    "* A notebook submission is required per group, no multiple submissions from same group. Submit by attaching only the .ipynb file please, not zip and I don't need other files (e.g the data) I already have them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ccSQ7-ga-nN"
   },
   "source": [
    "Group \n",
    "\n",
    "    Mohammed Amidu\n",
    "    Dilys Dickson\n",
    "    Joseph Marie\n",
    "    Vangelis Oden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tccnBmuta-nP"
   },
   "source": [
    "This task is inspired by the Audio Classifier Tutorial (found [here](https://pytorch.org/tutorials/beginner/audio_classifier_tutorial.html?highlight=audio))\n",
    "**Author**: `Winston Herring <https://github.com/winston6>`_\n",
    "\n",
    "This tutorial will show you how to correctly format an audio dataset and\n",
    "then train/test an audio classifier network on the dataset. First, let’s\n",
    "import the common torch packages as well as ``torch``, ``torchaudio``, ``pandas``,\n",
    "and ``numpy``. ``torchaudio`` is available `here <https://github.com/pytorch/audio>`_\n",
    "and can be installed by following the\n",
    "instructions on the website but I encountered a lot of trouble installing it, this ` conda install -c pytorch torchaudio-cpu ` rather worked for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GqY4gXt6a-nR"
   },
   "source": [
    "If you have a CUDA GPU use that. Running\n",
    "the network on a GPU will greatly decrease the training/testing runtime.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFN_en3na-nT"
   },
   "source": [
    "1: Importing the Dataset\n",
    "---------------------\n",
    "\n",
    "We will use the UrbanSound8K dataset to train our network. It is\n",
    "available for free `here <https://urbansounddataset.weebly.com/>`_ and contains\n",
    "10 audio classes with over 8000 audio samples! Once you have downloaded\n",
    "the compressed dataset, extract it to your current working directory.\n",
    "First, we will look at the csv file that provides information about the\n",
    "individual sound files. ``pandas`` allows us to open the csv file and\n",
    "use ``.iloc()`` to access the data within it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n7uGHGqya-nV"
   },
   "source": [
    "The 10 audio classes in the UrbanSound8K dataset are air_conditioner,\n",
    "car_horn, children_playing, dog_bark, drilling, enginge_idling,\n",
    "gun_shot, jackhammer, siren, and street_music. Let’s play a couple files\n",
    "and see what they sound like. The first file is street music and the\n",
    "second is an air conditioner.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VFA_-Gbaa-nW"
   },
   "source": [
    "2: Formatting the Data\n",
    "-------------------\n",
    "\n",
    "Now that we know the format of the csv file entries, we can construct\n",
    "our dataset. We will create a rapper class for our dataset using\n",
    "``torch.utils.data.Dataset`` that will handle loading the files and\n",
    "performing some formatting steps. The UrbanSound8K dataset is separated\n",
    "into 10 folders. We will use the data from 9 of these folders to train\n",
    "our network and then use the 10th folder to test the network. The rapper\n",
    "class will store the file names, labels, and folder numbers of the audio\n",
    "files in the inputted folder list when initialized. The actual loading\n",
    "and formatting steps will happen in the access function ``__getitem__``.\n",
    "\n",
    "In ``__getitem__``, we use ``torchaudio.load()`` to convert the wav\n",
    "files to tensors. ``torchaudio.load()`` returns a tuple containing the\n",
    "newly created tensor along with the sampling frequency of the audio file\n",
    "(44.1kHz for UrbanSound8K). The dataset uses two channels for audio so\n",
    "we will use ``torchaudio.transforms.DownmixMono()`` (not available in the latest version of `torchaudio`) to convert the audio\n",
    "data to one channel. Next, we need to format the audio data. The network\n",
    "we will make takes an input size of 32,000, while most of the audio\n",
    "files have well over 100,000 samples. The UrbanSound8K audio is sampled\n",
    "at 44.1kHz, so 32,000 samples only covers around 700 milliseconds. By\n",
    "downsampling the audio to aproximately 8kHz, we can represent 4 seconds\n",
    "with the 32,000 samples. This downsampling is achieved by taking every\n",
    "fifth sample of the original audio tensor. Not every audio tensor is\n",
    "long enough to handle the downsampling so these tensors will need to be\n",
    "padded with zeros. The minimum length that won’t require padding is\n",
    "160,000 samples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pF6SineKa-nX"
   },
   "source": [
    "3: Define the Network\n",
    "------------------\n",
    "\n",
    "For this task we want to closely reproduce the achitectures described in https://arxiv.org/pdf/1610.00087.pdf. You task is to read extensively the paper and reproduce the achitectures <font color='green'> M3, M5, M11 and M18. The M34-res is a bonus.</font>\n",
    "While attempting to reproduce the architectures endeavour to read through the common [pitfalls](https://urbansounddataset.weebly.com/urbansound8k.html#10foldCV) to get it right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nlzm7r6da-nY"
   },
   "source": [
    "We will use the same optimization technique used in the paper, an Adam\n",
    "optimizer with weight decay set to 0.0001. At first, we will train with\n",
    "a learning rate of 0.01, but we will use a ``scheduler`` to decrease it\n",
    "to 0.001 during training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZoD7mez2a-nZ"
   },
   "source": [
    "4: Training and Testing the Network\n",
    "--------------------------------\n",
    "\n",
    "You can define a training function that will feed our training data into the model and perform the backward pass and optimization steps. You can also make one for testing the networks accuracy and set the model to ``eval()`` mode and then run inference on the test dataset. Calling ``eval()`` sets the training variable in all modules in the network to false. Certain layers like batch normalization and dropout layers behave differently during training so this step is crucial for getting correct results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CyPiVVe6a-na"
   },
   "source": [
    "Finally, we can train and test the network. Train the network for as many epochs as time allows you. The network will be tested after each epoch to see how the accuracy varies during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1zlZ6vLa-nb"
   },
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "If trained on 9 folders, the network should be about 40% accurate by the end of the training process for the least possible epochs. Training on less folders will result in a lower overall accuracy. Greater accuracies can be achieved using deeper CNNs at the expense of a larger memory footprint.\n",
    "\n",
    "For more advanced audio applications, such as speech recognition,\n",
    "recurrent neural networks (RNNs) are commonly used. There are also other\n",
    "data preprocessing methods, such as finding the mel frequency cepstral\n",
    "coefficients (MFCC), that can reduce the size of the dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7b1VWyva-nf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-_9VSXyqa-nn"
   },
   "outputs": [],
   "source": [
    "#rapper for the UrbanSound8K dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "yb3ZAR8RoD4T",
    "outputId": "5af486be-e6ba-4567-a7d1-4404c59a3ae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-13 11:14:30--  https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz\n",
      "Resolving zenodo.org (zenodo.org)... 188.184.95.95\n",
      "Connecting to zenodo.org (zenodo.org)|188.184.95.95|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6023741708 (5.6G) [application/octet-stream]\n",
      "Saving to: ‘UrbanSound8K.tar.gz’\n",
      "\n",
      "UrbanSound8K.tar.gz 100%[===================>]   5.61G  47.0MB/s    in 1m 52s  \n",
      "\n",
      "2019-12-13 11:16:23 (51.4 MB/s) - ‘UrbanSound8K.tar.gz’ saved [6023741708/6023741708]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz\n",
    "!tar -xf UrbanSound8K.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "WHlbMCiEsgiR",
    "outputId": "dbc2f885-bdf3-4f4a-ed61-da7ac0d4f947",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
      "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.17.4)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.8)\n",
      "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.14.0)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.1)\n",
      "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.12.0)\n",
      "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
      "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.3.3)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.21.3)\n",
      "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.30.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "id": "sFrDvDCwa-n6",
    "outputId": "3cde094b-3245-4ed3-c721-64175d8e3e29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Useful Modules\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from keras import layers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation, BatchNormalization, Conv1D, Dense, GlobalAveragePooling1D, Input, MaxPooling1D, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "from random import choice\n",
    "from time import time\n",
    "from glob import iglob\n",
    "from shutil import rmtree\n",
    "import librosa\n",
    "import numpy as np\n",
    "import sys\n",
    "from glob import glob\n",
    "from sklearn.model_selection import KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9D4d4lea-oA"
   },
   "outputs": [],
   "source": [
    "# Set Audio Paths\n",
    "audio_path = '/home/michael/Documents/Programming#/Assignments#/UrbanSound8K/audio'\n",
    "TARGET_SR = 8000\n",
    "output_path = '/home/michael/Documents/Programming#/Assignments#/UrbanSound8K/Waveforms#'\n",
    "output_path_train = os.path.join(output_path, 'train')\n",
    "output_path_test = os.path.join(output_path, 'test')\n",
    "\n",
    "audio_size = 32000\n",
    "\n",
    "# Print trainable parameters from models\n",
    "def print_total_trainable_parameters_count():\n",
    "    import tensorflow as tf\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        print(shape)\n",
    "        # print(len(shape))\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            # print(dim)\n",
    "            variable_parameters *= dim.value\n",
    "        # print(variable_parameters)\n",
    "        total_parameters += variable_parameters\n",
    "    print(total_parameters)\n",
    "\n",
    "def print_delimiter():\n",
    "    print('-' * 80)\n",
    "\n",
    "def mkdir_p(path):\n",
    "    import errno\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def del_folder(path):\n",
    "    try:\n",
    "        rmtree(path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Extract classes from the dataset\n",
    "def extract_class_id(wav_filename):\n",
    "    \"\"\"\n",
    "    The name of the audio file. The name takes the following format: [fsID]-[classID]-[occurrenceID]-[sliceID].wav, where:\n",
    "    [fsID] = the Freesound ID of the recording from which this excerpt (slice) is taken\n",
    "    [classID] = a numeric identifier of the sound class (see description of classID below for further details)\n",
    "    [occurrenceID] = a numeric identifier to distinguish different occurrences of the sound within the original recording\n",
    "    [sliceID] = a numeric identifier to distinguish different slices taken from the same occurrence\n",
    "    \"\"\"\n",
    "    return wav_filename.split('-')[1]\n",
    "\n",
    "# Convert the dataset from the folders.\n",
    "def convert_data():\n",
    "    for i, wav_filename in enumerate(iglob(os.path.join(audio_path, '**/**.wav'), recursive=True)):\n",
    "        class_id = extract_class_id(wav_filename)\n",
    "        audio_buf = read_audio_from_filename(wav_filename, target_sr=TARGET_SR)\n",
    "        # normalize mean 0, variance 1\n",
    "        audio_buf = (audio_buf - np.mean(audio_buf)) / np.std(audio_buf)\n",
    "        original_length = len(audio_buf)\n",
    "        print(i, wav_filename, original_length, np.round(np.mean(audio_buf), 4), np.std(audio_buf))\n",
    "        if original_length < audio_size:\n",
    "            audio_buf = np.concatenate((audio_buf, np.zeros(shape=(audio_size - original_length, 1))))\n",
    "            print('PAD New length =', len(audio_buf))\n",
    "        elif original_length > audio_size:\n",
    "            audio_buf = audio_buf[0:audio_size]\n",
    "            print('CUT New length =', len(audio_buf))\n",
    "\n",
    "        output_folder = output_path_train\n",
    "        if 'fold10' in wav_filename:\n",
    "            output_folder = output_path_test\n",
    "        output_filename = os.path.join(output_folder, str(i) + '.pkl')\n",
    "\n",
    "        out = {'class_id': class_id,\n",
    "               'audio': audio_buf,\n",
    "               'sr': TARGET_SR}\n",
    "        with open(output_filename, 'wb') as w:\n",
    "            pickle.dump(out, w)\n",
    "\n",
    "def read_audio_from_filename(filename, target_sr):\n",
    "    audio, _ = librosa.load(filename, sr=target_sr, mono=True)\n",
    "    audio = audio.reshape(-1, 1)\n",
    "    return audio\n",
    "\n",
    "\n",
    "def next_batch_blank(batch_size):\n",
    "    return np.zeros(shape=(batch_size, audio_size, 1), dtype=np.float32), np.ones(shape=batch_size)\n",
    "\n",
    "def conv_pattern_extractor(pattern):\n",
    "    pattern = pattern.replace('x', '*')\n",
    "    num_blocks = 1\n",
    "    if '*' in pattern:\n",
    "        pattern, p2 = pattern.split('*')\n",
    "        num_blocks = int(p2)\n",
    "    pattern = pattern.strip()[1:-1]\n",
    "    p1, p2 = pattern.split(',')\n",
    "    nb_filters = int(p2)\n",
    "    if '/' in p1:\n",
    "        receptive_field, strides = [int(v) for v in p1.split('/')]\n",
    "    else:\n",
    "        receptive_field = int(p1)\n",
    "        strides = 1\n",
    "    print(nb_filters, receptive_field, strides, num_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGUFfqHJa-oE"
   },
   "outputs": [],
   "source": [
    "# Class for logging our outputs to local machine.\n",
    "class FileLogger(object):\n",
    "    def __init__(self, full_filename, headers):\n",
    "        self._headers = headers\n",
    "        self._out_fp = open(full_filename, 'w')\n",
    "        self._write(headers)\n",
    "\n",
    "    def write(self, line):\n",
    "        assert len(line) == len(self._headers)\n",
    "        self._write(line)\n",
    "\n",
    "    def close(self):\n",
    "        self._out_fp.close()\n",
    "\n",
    "    def _write(self, arr):\n",
    "        arr = [str(e) for e in arr]\n",
    "        self._out_fp.write(' '.join(arr) + '\\n')\n",
    "        self._out_fp.flush()\n",
    "\n",
    "# Create a datareader class to Read the data and move it to batches\n",
    "class DataReader:\n",
    "    def __init__(self):\n",
    "        self.train_files = glob(os.path.join(output_path_train, '**.pkl'))\n",
    "        print('training files =', len(self.train_files))\n",
    "        self.test_files = glob(os.path.join(output_path_test, '**.pkl'))\n",
    "        print('testing files =', len(self.test_files))\n",
    "\n",
    "    def next_batch_train(self, batch_size):\n",
    "        return DataReader._next_batch(batch_size, self.train_files)\n",
    "\n",
    "    def next_batch_test(self, batch_size):\n",
    "        return DataReader._next_batch(batch_size, self.test_files)\n",
    "\n",
    "    def train_files_count(self):\n",
    "        return len(self.train_files)\n",
    "\n",
    "    def test_files_count(self):\n",
    "        return len(self.test_files)\n",
    "\n",
    "    def get_all_training_data(self):\n",
    "        return DataReader._get_data(self.train_files)\n",
    "\n",
    "    def get_all_testing_data(self):\n",
    "        return DataReader._get_data(self.test_files)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_data(file_list, progress_bar=False):\n",
    "        def load_into(_filename, _x, _y):\n",
    "            with open(_filename, 'rb') as f:\n",
    "                audio_element = pickle.load(f)\n",
    "                _x.append(audio_element['audio'])\n",
    "                _y.append(int(audio_element['class_id']))\n",
    "\n",
    "        x, y = [], []\n",
    "        for filename in file_list:\n",
    "            load_into(filename, x, y)\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    @staticmethod\n",
    "    def _next_batch(batch_size, file_list):\n",
    "        return DataReader._get_data([choice(file_list) for _ in range(batch_size)])\n",
    "\n",
    "# Return all metrics\n",
    "class MetricsHistory(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        file_logger.write([str(epoch),\n",
    "                           str(logs['loss']),\n",
    "                           str(logs['val_loss']),\n",
    "                           str(logs['accuracy']),\n",
    "                           str(logs['val_accuracy'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWGMrDwSa-oT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vNXM22KPa-os"
   },
   "outputs": [],
   "source": [
    "# Define M3 Network\n",
    "def m3(num_classes=10):\n",
    "    print('Using Model M3')\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(256,\n",
    "                 input_shape=[audio_size, 1],\n",
    "                 kernel_size=80,\n",
    "                 strides=4,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "    m.add(Conv1D(256,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Activation('relu'))\n",
    "\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "    m.add(Lambda(lambda x: K.mean(x, axis=1))) # Same as Global Average Pooling for 1D Conv Layer\n",
    "    m.add(Dense(num_classes, activation='softmax')) # Dense Layers.\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AeVUUnCaCzb"
   },
   "outputs": [],
   "source": [
    "# Define M5 model \n",
    "def m5(num_classes=10):\n",
    "    print('Using Model M5')\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(128,\n",
    "                 input_shape=[audio_size, 1],\n",
    "                 kernel_size=80,\n",
    "                 strides=4,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "    m.add(Conv1D(128,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "    m.add(Conv1D(256,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "    m.add(Conv1D(512,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "    m.add(Lambda(lambda x: K.mean(x, axis=1))) \n",
    "    m.add(Dense(num_classes, activation='softmax'))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KjVwQ4C6aGQZ"
   },
   "outputs": [],
   "source": [
    "# Define M11 network\n",
    "def m11(num_classes=10):\n",
    "    print('Using Model M11')\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(64,\n",
    "                 input_shape=[audio_size, 1],\n",
    "                 kernel_size=80,\n",
    "                 strides=4,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "\n",
    "    for i in range(2):\n",
    "        m.add(Conv1D(64,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='glorot_uniform',\n",
    "                     kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "        m.add(BatchNormalization())\n",
    "        m.add(Activation('relu'))\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "\n",
    "    for i in range(2):\n",
    "        m.add(Conv1D(128,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='glorot_uniform',\n",
    "                     kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "        m.add(BatchNormalization())\n",
    "        m.add(Activation('relu'))\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "\n",
    "    for i in range(3):\n",
    "        m.add(Conv1D(256,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='glorot_uniform',\n",
    "                     kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "        m.add(BatchNormalization())\n",
    "        m.add(Activation('relu'))\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "\n",
    "    for i in range(2):\n",
    "        m.add(Conv1D(512,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='glorot_uniform',\n",
    "                     kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "        m.add(BatchNormalization())\n",
    "        m.add(Activation('relu'))\n",
    "\n",
    "    m.add(Lambda(lambda x: K.mean(x, axis=1)))\n",
    "    m.add(Dense(num_classes, activation='softmax'))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "th6MMK9UaJAP"
   },
   "outputs": [],
   "source": [
    "# Define M18\n",
    "def m18(num_classes=10):\n",
    "    print('Using Model M18')\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(64,\n",
    "                 input_shape=[audio_size, 1],\n",
    "                 kernel_size=80,\n",
    "                 strides=4,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "\n",
    "    for i in range(4):\n",
    "        m.add(Conv1D(64,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='glorot_uniform',\n",
    "                     kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "        m.add(BatchNormalization())\n",
    "        m.add(Activation('relu'))\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "\n",
    "    for i in range(4):\n",
    "        m.add(Conv1D(128,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='glorot_uniform',\n",
    "                     kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "        m.add(BatchNormalization())\n",
    "        m.add(Activation('relu'))\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "\n",
    "    for i in range(4):\n",
    "        m.add(Conv1D(256,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='glorot_uniform',\n",
    "                     kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "        m.add(BatchNormalization())\n",
    "        m.add(Activation('relu'))\n",
    "    m.add(MaxPooling1D(pool_size=4, strides=None))\n",
    "\n",
    "    for i in range(4):\n",
    "        m.add(Conv1D(512,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='glorot_uniform',\n",
    "                     kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "        m.add(BatchNormalization())\n",
    "        m.add(Activation('relu'))\n",
    "\n",
    "    m.add(Lambda(lambda x: K.mean(x, axis=1))) \n",
    "    m.add(Dense(num_classes, activation='softmax'))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcDoYmZFaMKf"
   },
   "outputs": [],
   "source": [
    "\n",
    "# For m34 Residual, use RepeatVector. Or tensorflow backend.repeat\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    conv_name_base = 'res' + str(stage) + str(block) + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + str(block) + '_branch'\n",
    "\n",
    "    x = Conv1D(filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=1,\n",
    "               padding='same',\n",
    "               kernel_initializer='glorot_uniform',\n",
    "               kernel_regularizer=regularizers.l2(l=0.0001),\n",
    "               name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv1D(filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=1,\n",
    "               padding='same',\n",
    "               kernel_initializer='glorot_uniform',\n",
    "               kernel_regularizer=regularizers.l2(l=0.0001),\n",
    "               name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2b')(x)\n",
    "\n",
    "    # up-sample from the activation maps.\n",
    "    # here we x2 the number of filters.\n",
    "    # See that as duplicating everything and concatenate them.\n",
    "    if input_tensor.shape[2] != x.shape[2]:\n",
    "        x = layers.add([x, Lambda(lambda y: K.repeat_elements(y, rep=2, axis=2))(input_tensor)])\n",
    "    else:\n",
    "        x = layers.add([x, input_tensor])\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_34(num_classes=10):\n",
    "    inputs = Input(shape=[audio_size, 1])\n",
    "\n",
    "    x = Conv1D(48,\n",
    "               kernel_size=80,\n",
    "               strides=4,\n",
    "               padding='same',\n",
    "               kernel_initializer='glorot_uniform',\n",
    "               kernel_regularizer=regularizers.l2(l=0.0001))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=4, strides=None)(x)\n",
    "\n",
    "    for i in range(3):\n",
    "        x = identity_block(x, kernel_size=3, filters=48, stage=1, block=i)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=4, strides=None)(x)\n",
    "\n",
    "    for i in range(4):\n",
    "        x = identity_block(x, kernel_size=3, filters=96, stage=2, block=i)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=4, strides=None)(x)\n",
    "\n",
    "    for i in range(6):\n",
    "        x = identity_block(x, kernel_size=3, filters=192, stage=3, block=i)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=4, strides=None)(x)\n",
    "\n",
    "    for i in range(3):\n",
    "        x = identity_block(x, kernel_size=3, filters=384, stage=4, block=i)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    m = Model(inputs, x, name='resnet34')\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dtu0jUrIeJkk",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training files = 7895\n",
      "testing files = 837\n",
      "0.08637261390686035 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[-0.28862223],\n",
       "         [ 0.33379519],\n",
       "         [ 1.05820024],\n",
       "         ...,\n",
       "         [ 1.00229537],\n",
       "         [-0.34486553],\n",
       "         [-0.48807079]],\n",
       " \n",
       "        [[-0.85831708],\n",
       "         [-1.31762409],\n",
       "         [-0.20709692],\n",
       "         ...,\n",
       "         [-0.92532337],\n",
       "         [-0.78980714],\n",
       "         [-1.08291662]],\n",
       " \n",
       "        [[ 0.02341719],\n",
       "         [ 0.37758783],\n",
       "         [ 0.878241  ],\n",
       "         ...,\n",
       "         [-3.03388095],\n",
       "         [-3.08291292],\n",
       "         [-3.30102062]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.11735019],\n",
       "         [ 0.16110203],\n",
       "         [ 1.01452184],\n",
       "         ...,\n",
       "         [ 0.18869561],\n",
       "         [ 0.06019786],\n",
       "         [ 1.00366247]],\n",
       " \n",
       "        [[ 1.83861363],\n",
       "         [ 2.30447245],\n",
       "         [-0.09918652],\n",
       "         ...,\n",
       "         [-0.12858652],\n",
       "         [-0.08820677],\n",
       "         [ 0.03323535]],\n",
       " \n",
       "        [[ 0.65142566],\n",
       "         [ 0.48577085],\n",
       "         [-0.57317448],\n",
       "         ...,\n",
       "         [-0.10246004],\n",
       "         [-0.47736081],\n",
       "         [ 1.81131577]]]),\n",
       " array([2, 9, 5, 5, 5, 5, 6, 2, 5, 0, 3, 8, 5, 5, 3, 3, 5, 5, 9, 9, 4, 0,\n",
       "        4, 2, 7, 1, 2, 3, 4, 7, 3, 2]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del_folder(output_path_train)\n",
    "# del_folder(output_path_test)\n",
    "# mkdir_p(output_path_train)\n",
    "# mkdir_p(output_path_test)\n",
    "\n",
    "# conv_pattern_extractor('[80/4, 256] x 2')\n",
    "# conv_pattern_extractor('[80/4, 256]')\n",
    "# conv_pattern_extractor('[3, 512]')\n",
    "\n",
    "# Run convert_data to downsample and upsample the data\n",
    "# convert_data()\n",
    "data_reader = DataReader()\n",
    "a = time()\n",
    "data_reader.next_batch_train(128)\n",
    "print(time() - a, 'sec')\n",
    "data_reader.next_batch_test(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdUTE1LCeOkR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model selected: m3\n",
      "Using Model M3\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 8000, 256)         20736     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8000, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8000, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 2000, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 2000, 256)         196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2000, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2000, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 500, 256)          0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 222,218\n",
      "Trainable params: 221,194\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n",
      "training files = 7895\n",
      "testing files = 837\n",
      "x_tr.shape = (7895, 32000, 1)\n",
      "y_tr.shape = (7895, 10)\n",
      "x_te.shape = (837, 32000, 1)\n",
      "y_te.shape = (837, 10)\n",
      "Train on 7105 samples, validate on 790 samples\n",
      "Epoch 1/10\n",
      "7105/7105 [==============================] - 510s 72ms/step - loss: 1.5809 - accuracy: 0.4552 - val_loss: 2.3209 - val_accuracy: 0.1127\n",
      "Epoch 2/10\n",
      "7105/7105 [==============================] - 513s 72ms/step - loss: 1.3257 - accuracy: 0.5512 - val_loss: 2.5782 - val_accuracy: 0.1101\n",
      "Epoch 3/10\n",
      "7105/7105 [==============================] - 514s 72ms/step - loss: 1.2404 - accuracy: 0.5859 - val_loss: 2.4372 - val_accuracy: 0.1646\n",
      "Epoch 4/10\n",
      "7105/7105 [==============================] - 521s 73ms/step - loss: 1.1949 - accuracy: 0.5975 - val_loss: 1.9232 - val_accuracy: 0.3354\n",
      "Epoch 5/10\n",
      "7105/7105 [==============================] - 522s 73ms/step - loss: 1.1235 - accuracy: 0.6296 - val_loss: 1.6545 - val_accuracy: 0.4418\n",
      "Epoch 6/10\n",
      "7105/7105 [==============================] - 536s 76ms/step - loss: 1.0847 - accuracy: 0.6539 - val_loss: 1.3658 - val_accuracy: 0.5367\n",
      "Epoch 7/10\n",
      "7105/7105 [==============================] - 525s 74ms/step - loss: 1.0502 - accuracy: 0.6645 - val_loss: 1.2525 - val_accuracy: 0.5886\n",
      "Epoch 8/10\n",
      "7105/7105 [==============================] - 636s 89ms/step - loss: 1.0256 - accuracy: 0.6709 - val_loss: 1.4466 - val_accuracy: 0.5228\n",
      "Epoch 9/10\n",
      "7105/7105 [==============================] - 611s 86ms/step - loss: 1.0025 - accuracy: 0.6781 - val_loss: 1.1745 - val_accuracy: 0.6354\n",
      "Epoch 10/10\n",
      "7105/7105 [==============================] - 513s 72ms/step - loss: 0.9769 - accuracy: 0.6884 - val_loss: 1.3716 - val_accuracy: 0.5278\n",
      "Train on 7105 samples, validate on 790 samples\n",
      "Epoch 1/10\n",
      "7105/7105 [==============================] - 507s 71ms/step - loss: 0.9510 - accuracy: 0.7033 - val_loss: 1.1445 - val_accuracy: 0.6101\n",
      "Epoch 2/10\n",
      "7105/7105 [==============================] - 512s 72ms/step - loss: 0.9357 - accuracy: 0.7022 - val_loss: 1.0261 - val_accuracy: 0.6582\n",
      "Epoch 3/10\n",
      "7105/7105 [==============================] - 521s 73ms/step - loss: 0.9192 - accuracy: 0.7088 - val_loss: 1.1281 - val_accuracy: 0.6101\n",
      "Epoch 4/10\n",
      "7105/7105 [==============================] - 573s 81ms/step - loss: 0.9062 - accuracy: 0.7139 - val_loss: 1.3567 - val_accuracy: 0.5367\n",
      "Epoch 5/10\n",
      "7105/7105 [==============================] - 642s 90ms/step - loss: 0.8915 - accuracy: 0.7202 - val_loss: 0.9680 - val_accuracy: 0.6797\n",
      "Epoch 6/10\n",
      "7105/7105 [==============================] - 599s 84ms/step - loss: 0.8591 - accuracy: 0.7329 - val_loss: 1.0213 - val_accuracy: 0.6582\n",
      "Epoch 7/10\n",
      "7105/7105 [==============================] - 515s 72ms/step - loss: 0.8674 - accuracy: 0.7261 - val_loss: 1.1282 - val_accuracy: 0.6380\n",
      "Epoch 8/10\n",
      "7105/7105 [==============================] - 513s 72ms/step - loss: 0.8422 - accuracy: 0.7444 - val_loss: 0.9638 - val_accuracy: 0.6924\n",
      "Epoch 9/10\n",
      "7105/7105 [==============================] - 522s 73ms/step - loss: 0.8533 - accuracy: 0.7309 - val_loss: 1.1994 - val_accuracy: 0.5810\n",
      "Epoch 10/10\n",
      "7105/7105 [==============================] - 514s 72ms/step - loss: 0.8361 - accuracy: 0.7382 - val_loss: 0.9213 - val_accuracy: 0.6924\n",
      "Train on 7105 samples, validate on 790 samples\n",
      "Epoch 1/10\n",
      "7105/7105 [==============================] - 524s 74ms/step - loss: 0.8210 - accuracy: 0.7462 - val_loss: 1.2029 - val_accuracy: 0.5873\n",
      "Epoch 2/10\n",
      "7105/7105 [==============================] - 517s 73ms/step - loss: 0.8148 - accuracy: 0.7416 - val_loss: 1.0406 - val_accuracy: 0.6696\n",
      "Epoch 3/10\n",
      "7105/7105 [==============================] - 551s 77ms/step - loss: 0.8038 - accuracy: 0.7541 - val_loss: 1.2316 - val_accuracy: 0.5835\n",
      "Epoch 4/10\n",
      "7105/7105 [==============================] - 576s 81ms/step - loss: 0.7883 - accuracy: 0.7534 - val_loss: 1.0781 - val_accuracy: 0.6380\n",
      "Epoch 5/10\n",
      "7105/7105 [==============================] - 549s 77ms/step - loss: 0.7728 - accuracy: 0.7626 - val_loss: 1.1589 - val_accuracy: 0.6241\n",
      "Epoch 6/10\n",
      "7105/7105 [==============================] - 510s 72ms/step - loss: 0.7786 - accuracy: 0.7551 - val_loss: 1.0825 - val_accuracy: 0.6544\n",
      "Epoch 7/10\n",
      "7105/7105 [==============================] - 511s 72ms/step - loss: 0.7765 - accuracy: 0.7640 - val_loss: 0.9993 - val_accuracy: 0.6671\n",
      "Epoch 8/10\n",
      "7105/7105 [==============================] - 509s 72ms/step - loss: 0.7544 - accuracy: 0.7661 - val_loss: 0.8411 - val_accuracy: 0.7291\n",
      "Epoch 9/10\n",
      "7105/7105 [==============================] - 500s 70ms/step - loss: 0.7511 - accuracy: 0.7673 - val_loss: 1.1068 - val_accuracy: 0.6076\n",
      "Epoch 10/10\n",
      "7105/7105 [==============================] - 497s 70ms/step - loss: 0.7473 - accuracy: 0.7692 - val_loss: 1.2209 - val_accuracy: 0.5886\n",
      "Train on 7105 samples, validate on 790 samples\n",
      "Epoch 1/10\n",
      "7105/7105 [==============================] - 501s 70ms/step - loss: 0.7431 - accuracy: 0.7747 - val_loss: 0.8772 - val_accuracy: 0.7013\n",
      "Epoch 2/10\n",
      "7105/7105 [==============================] - 521s 73ms/step - loss: 0.7316 - accuracy: 0.7758 - val_loss: 1.2517 - val_accuracy: 0.6253\n",
      "Epoch 3/10\n",
      "2432/7105 [=========>....................] - ETA: 5:38 - loss: 0.7177 - accuracy: 0.7755"
     ]
    }
   ],
   "source": [
    "\n",
    "model_names = ['m3', 'm5', 'm11', 'm18', 'm34']\n",
    "for i in model_names:\n",
    "    model_name = i\n",
    "    args = sys.argv\n",
    "    if len(args) == 2:\n",
    "        model_name = args[1].lower()\n",
    "    print('Model selected:', model_name)\n",
    "    file_logger = FileLogger('out_{}.tsv'.format(model_name), ['step', 'train_loss', 'test_loss',\n",
    "                                                               'train_acc', 'test_acc'])\n",
    "\n",
    "    num_classes = 10\n",
    "    if model_name == 'm3':\n",
    "        model = m3(num_classes=num_classes)\n",
    "    elif model_name == 'm5':\n",
    "        model = m5(num_classes=num_classes)\n",
    "    elif model_name == 'm11':\n",
    "        model = m11(num_classes=num_classes)\n",
    "    elif model_name == 'm18':\n",
    "        model = m18(num_classes=num_classes)\n",
    "    elif model_name == 'm34':\n",
    "        model = resnet_34(num_classes=num_classes)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    data_reader = DataReader()\n",
    "    x_tr, y_tr = data_reader.get_all_training_data()\n",
    "    y_tr = to_categorical(y_tr, num_classes=num_classes)\n",
    "    x_te, y_te = data_reader.get_all_testing_data()\n",
    "    y_te = to_categorical(y_te, num_classes=num_classes)\n",
    "\n",
    "    print('x_tr.shape =', x_tr.shape)\n",
    "    print('y_tr.shape =', y_tr.shape)\n",
    "    print('x_te.shape =', x_te.shape)\n",
    "    print('y_te.shape =', y_te.shape)\n",
    "\n",
    "    # Reduce the learning rate by half if the accuracy does not increase after 10 epochs\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=10, min_lr=0.0001, verbose=1)\n",
    "    metrics_history = MetricsHistory()\n",
    "    batch_size = 128\n",
    "    n_split = 10\n",
    "    test_scores = []\n",
    "\n",
    "    # Cross Validation for the training\n",
    "    for train_index, val_index in KFold(n_split).split(x_tr):\n",
    "        x_train, x_val = x_tr[train_index], x_tr[val_index]\n",
    "        y_train, y_val = y_tr[train_index], y_tr[val_index]\n",
    "        model.fit(x=x_train,\n",
    "                  y=y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=10,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  validation_data=(x_val, y_val),\n",
    "                  callbacks=[metrics_history, reduce_lr])\n",
    "        scores = model.evaluate(x_te, y_te, verbose=0)\n",
    "        test_scores.append(scores[1] * 100)\n",
    "    print(\"Final Accuracy %.2f%% (+/- %.2f%%)\" % (numpy.mean(test_scores), numpy.std(test_scores)))\n",
    "\n",
    "    file_logger.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fL6tFXnoa-ov",
    "outputId": "d3333d25-e1c5-4152-e73e-f9d5fe7ded91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/michael/Documents/Programming#/Assignments#'"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0joNtHEa-o7",
    "outputId": "304ca6f0-14ae-4199-8430-2623b9d18f1c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.541343</td>\n",
       "      <td>2.390405</td>\n",
       "      <td>0.473211</td>\n",
       "      <td>0.118280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.306810</td>\n",
       "      <td>2.663871</td>\n",
       "      <td>0.559721</td>\n",
       "      <td>0.133811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.212127</td>\n",
       "      <td>2.484400</td>\n",
       "      <td>0.600887</td>\n",
       "      <td>0.238949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.151480</td>\n",
       "      <td>2.022581</td>\n",
       "      <td>0.625332</td>\n",
       "      <td>0.290323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.115919</td>\n",
       "      <td>1.857373</td>\n",
       "      <td>0.633692</td>\n",
       "      <td>0.397849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step  train_loss  test_loss  train_acc  test_acc\n",
       "0     0    1.541343   2.390405   0.473211  0.118280\n",
       "1     1    1.306810   2.663871   0.559721  0.133811\n",
       "2     2    1.212127   2.484400   0.600887  0.238949\n",
       "3     3    1.151480   2.022581   0.625332  0.290323\n",
       "4     4    1.115919   1.857373   0.633692  0.397849"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_m3 = pd.read_csv('out_m31.csv')\n",
    "data_m3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbDeh0d5a-pA",
    "outputId": "8ef1a279-a075-4043-e301-3b671fd60776"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f721b10ca58>]"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Creates two subplots and unpacks the output array immediately\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=False, figsize= (10, 6))\n",
    "ax1.plot(data_m3.train_loss)\n",
    "ax1.plot(data_m3.test_loss)\n",
    "ax1.set_title('Train and Test Losses')\n",
    "ax2.set_title('Train and Test Accuracy')\n",
    "ax2.plot(data_m3.train_acc)\n",
    "ax2.plot(data_m3.test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObIINsGNa-pH"
   },
   "outputs": [],
   "source": [
    "audio = torch.zeros(1,32000)\n",
    "m3(audio.unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cH8tNeaoa-pa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "audio_classifier_task-Tensorflow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
